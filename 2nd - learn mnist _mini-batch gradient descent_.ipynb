{
    "nbformat_minor": 2, 
    "cells": [
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Author: Andi Sama\n# Organization: Sinergi Wahana Gemilang\n#   a Value Added Distributor in Jakarta, Indonesia\n# Created: June 18, 2018\n# Last modified:\n#   - June 19, 2018\n#     * Add a few more information during training by modifying code in network_asm.py\n#       (% on classification rate & elapsed time per epoch + total elapsed time for all epochs)\n#     * Add a few more information during training by modifying code in mnist_loader_asm.py\n#       (loaded data sizes)\n#   - June 21, 2018\n#     * Apply cross-entropy on cost function (previously: use quadratic cost) to improve accuracy\n#     * Apply Mini-batch gradient descent (previously: just Stochastic Gradient Descent)\n#     * Add monitor_elapsed_time flag in network2_asm.py\n#     * Draw plot for training cost, evaluation cost against # of epoch\n#     * Use Gaussian distributions with mean 0 and standard deviation 1 over the square root\n#       of the number of weights connecting to the same neuron for network initialization\n#       (previously: Gaussian distribution without square root)\n#   - June 23, 2018\n#     * Use regularization - lambda \n#     * Visualization of mnist dataset\n#   - June 24, 2018\n#       Add predict() function use existing function load() network after save()\n#       => vanilla version done\n#   - June 24, 2018\n#     * use mnist testing dataset to predict (now still using training data)\n#   - June 25-26, 2018\n#     * migrate to IBM Watson Studio (on cloud)\n#       - upload all files to IBM object storage\n#       - get all files to working directory by using IBM boto3 api\n#       - convert code from python 2.7 to 3.5 (print function, cPickle handler, xrange)\n# Topic: Beginning Deep Learning (computer vision)\n# Purpose: Recognizing handwritten digits 0-9 in 3 steps\n#   - 1. Prepare datasets (MNIST, modified nist database)\n#   - 2. Train, Validate on datasets to generate deep learning model (architecture, weights)\n#   - 3. Predict new data based on generated model with testing data \n# Type of Neural Network: Supervised Learning with shallow neural network\n#   Dataset: mnist\n#     - original training dataset (60,000): handwritten digits from 250 persons (2 groups)\n#       further then divided to be 50,000 for training dataset and 10,000 for validation dataset\n#     - testing dataset (10,000): handwritten digits from another set of 250 persons (2 groups)\n# Hardware & Software platforms:\n#   - environment: cygwin on Windows, on Thinkpad T450 (i7 4 vCPUs without GPUs, 16 GB RAM)\n#   - programming language: python (v2.7)\n#   - deep learning framework: none\n#   - libraries: sys, pandas, numpy, math, matplotlib\n# Reference:\n#   - Michael A. Nielsen, \"Neural Network & Deep Learning\",\n#     Determination Press, 2015 http://neuralnetworksanddeeplearning.com \n#     Original code from the book is at https://github.com/mnielsen/neural-networks-and-deep-learning\n#   - various references from stackoverflow, github, and related library documentations etc"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# GET ALL REQUIRED FILES to working directory by using IBM boto3 api\n# Just need to do this once, comment afterward\n#\nimport sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\n# def __iter__(self): return 0"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Above cell is hidden for publishing... get your own key from IBM Watson Studio on IBM cloud to fill the following\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# replace \"your IBM API Key\" with your generated key\n# s3 = ibm_boto3.client(service_name='s3',\n#     ibm_api_key_id='your IBM API Key',\n#     ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n#     config=Config(signature_version='oauth'),\n#     endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# replace the following \"mybucket\" with your generated bucket\nmybucket='doctorofcomputerscience-donotdelete-pr-e1zj6xtk1pjpth'\n\n# 1. get mnist dataset\nwith open('mnist.pkl.gz', 'wb') as data:\n    s3.download_fileobj(mybucket, 'mnist.pkl.gz', data)\n\n# 2. get pyton library for loading mnist dataset\nwith open('mnist_loader_asm_py3.py', 'wb') as data:\n    s3.download_fileobj(mybucket, 'mnist_loader_asm_py3.py', data)\n    \n# 3. get pyton library for training & prediction\nwith open('network2_asm_b_py3.py', 'wb') as data:\n    s3.download_fileobj(mybucket, 'network2_asm_b_py3.py', data)\n\n# for testing, get trained network (without retraining)\nwith open('2018624223944_asm_Trained_NN', 'wb') as data:\n    s3.download_fileobj(mybucket, '2018624223944_asm_Trained_NN', data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls -al"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# !cat mnist_loader_asm_py3.py "
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sys\nimport matplotlib.pyplot as plt # graphic related things\nimport numpy as np\nimport math\nimport pandas as pd # dataframe manipulation\nimport mnist_loader_asm_py3 # helper program for loading mnist data set\nimport network2_asm_b_py3 # helper program, neural network layers"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#plot individual curve: training training/evaluation accuracy, training/evaluation cost\ndef plot_curve(xlabel, epoch, ylabel, loss):\n    # epoch: total epoch, loss: list of lost values for all epochs\n    epochs = []\n    for i in range(epoch):\n        epochs.append(i)\n    plt.plot(epochs,loss)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n    \n#compare training & evaluation accuracy / cost\ndef plot_curve_comparison(epoch, title, xlabel, data1, ylabel, data2):\n    epochs = []\n    for i in range(epoch):\n        epochs.append(i)\n    fig = plt.figure()\n    ax = plt.subplot(111)\n    ax.plot(epochs, data1, label=xlabel)\n    ax.plot(epochs, data2, label=ylabel)\n    plt.title(title)\n    ax.legend()\n    plt.show()"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": ">> Training dataset (x, y)], length: 2 arrays.\n  - 1st array (data, x) 28x28 pixels; total data: 50000 , length per data: 784\n  - 2nd array (target label, y) [0..9]; total data: 50000\n    sample dataset: 5\n"
                }
            ], 
            "source": "# ========================\n# 1. LOAD Dataset (MNIST)\n# ========================\n# 1a. load mnist data using mnist_loader library\n#   output is structured in such a way to fit and be ready for learning\n#   each data is in 784x1 dimension, reshape to 28x28 pixels for viewing\n#   each label is in 8x1 dimension,\n#     extract array content with \"1\" in it to get the actual label [\"0\"..\"9\"]\n#\n\n#training_data, validation_data, test_data = mnist_loader_asm_py3.load_data_wrapper()\nzip_training_data, zip_validation_data, zip_test_data = mnist_loader_asm_py3.load_data_wrapper()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "training_data = list(zip_training_data)\nvalidation_data = list(zip_validation_data)\ntest_data = list(zip_test_data)"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "A few samples for the 28x28 pixels mnist digit dataset with its corresponding label\n"
                }, 
                {
                    "ename": "NameError", 
                    "evalue": "name 'training_data' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-10-43ff04d25a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just resuffle so we are showing different data when refreshed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#get corresponding target label (y) \"digit\" from its vectorized format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "# 1b. just show a few sample digits with its corresponding labels\n#\nprint(\"A few samples for the 28x28 pixels mnist digit dataset with its corresponding label\")\nrow = 1\ncolumn = 3\nnp.random.shuffle(training_data) # just resuffle so we are showing different data when refreshed\nfor i in range(column):\n    #get corresponding target label (y) \"digit\" from its vectorized format\n    data = training_data[i][1]\n    digit=0\n    j=0\n    for x in data:\n        if int(x):\n            digit=j\n            break\n        else:\n            j=j+1\n    #then, plot the image data \n    image = training_data[i][0].reshape(28, 28)   \n    plt.subplot(row, column, i+1)  # subplot with size  \n    plt.imshow(image, cmap='gray')  # cmap='gray' is for black and white picture.\n    plt.title('labelled as = {0}'.format(digit))\n    plt.axis('on')  # do not show axis value\n    plt.tight_layout()  # automatic padding between subplots\n    filename = \"mnist_plot\"+str(i)+\".png\"\n    plt.savefig(filename)\n    plt.show()"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ==================================================\n# 2. TRAIN the Neural Network (SUPERVISED LEARNING)\n# ==================================================\n# 2a. first Create Neural Network Architecture (Input, Hidden Layers(), Output)\n# setting up input layers for 28 x 28 pixels mnist input image size = 784 input neurons\n# the following hyper-parameters are initially set\n#\ninput_neurons  = 784  # 1st layer, input layer\nhidden_neurons = 100  # 2nd layer, 1st hidden layers\noutput_neurons = 10  # nth layer, output layer\n# then define the network (now: 1 hidden layers; can be multiple)\nnet = network2_asm_b_py3.Network([input_neurons, hidden_neurons, output_neurons],\n    cost=network2_asm_b_py3.CrossEntropyCost)\nnet.default_weight_initializer()"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'training_data' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-12-ada3d7b87586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#           train acc:99.69%, eval acc:96.69%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m evaluation_cost, evaluation_accuracy, training_cost, training_accuracy =     net.SGD(training_data, epoch, mini_batch_size, lr, evaluation_data=test_data, lmbda=5,\n\u001b[0m\u001b[1;32m     46\u001b[0m     monitor_all=True)\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "# 2b. then, train the network (supervised learning)\n#  generate weights, biases for all defined neural network hidden nodes\n#\n# prepare to run with additional hyper-parameters\nlr = 0.25 # learning rate\nmini_batch_size = 16\nepoch = 50\n# the following training will take some few minutes to hours in total to run (on Thinkpad T450)\n#   using Back Propagation (Mini-batch GD - Mini-batch Gradient Descent). epoch starts at 0\n#   [experiment 1]: Total time for 30 Epochs are 669.534558058 seconds\n#     with learning rate = 0.5, 1st hidden neurons = 30 & mini batch size = 10, epoch = 30\n#       * at Epoch 0 - completed in 14.201 seconds\n#           train cost:0.592158354923, eval cost:0.583742260415\n#           train acc:91.148%, eval acc:91.38%\n#       * at Epoch 17 - completed in 12.691 seconds\n#           train cost:0.234122439643, eval cost:0.357925843064\n#           train acc:96.826%, eval acc:94.59%\n#       * at Epoch 29 - completed in 12.728 seconds\n#           train cost:0.197905372636, eval cost:0.380761454796\n#           train acc:97.424%, eval acc:94.91%\n#   [experiment 2]: Total time for 30 Epochs are 5535.98018217 seconds\n#     by keeping other parameters unchanged and change only 1st hidden neurons = 100  \n#       * at Epoch 0 - completed in 172.04 seconds\n#           train cost:0.483291977803, eval cost:0.505009341819\n#           train acc:93.266%, eval acc:92.85%\n#       * at Epoch 12 - completed in 162.898 seconds\n#           train cost:0.0828427459533, eval cost:0.285776222999\n#           train acc:99.206%, eval acc:96.7%\n#       * at Epoch 29 - completed in 167.119 seconds\n#           train cost:0.0162023169185, eval cost:0.331588846324\n#           train acc:99.94%, eval acc:96.78%\n#   [experiment 3]: Total time for 30 Epochs are 2042.2370491 seconds\n#     with learning rate changed to 0.25, 1st hidden neurons increased to 50\n#          keep mini batch size = 10 and epoch = 30, lambda (regularization) = 5\n#       * at Epoch 0 - completed in 97.323 seconds\n#           train cost:0.396410205065, eval cost:0.414407693281\n#           train acc:94.108%, eval acc:93.79%\n#       * at Epoch 16 - completed in 53.617 seconds\n#           train cost:0.0840871412076, eval cost:0.236337866115\n#           train acc:99.21%, eval acc:96.77%\n#       * at Epoch 29 - completed in 53.553 seconds\n#           train cost:0.0471638578835, eval cost:0.275443772384\n#           train acc:99.69%, eval acc:96.69%\n#\nevaluation_cost, evaluation_accuracy, training_cost, training_accuracy = \\\n    net.SGD(training_data, epoch, mini_batch_size, lr, evaluation_data=test_data, lmbda=5,\n    monitor_all=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 2c. save the last trained network sizes, weights, biases, cost\n#\n# generate current time to be embedded in filename\nimport time\ndt = time.localtime(time.time())\ni=0\nprefix = \"\"\nwhile i<6:\n    #print str(dt[i])\n    prefix = prefix + str(dt[i])\n    i=i+1\n# embed the current time in file name\nfilename = prefix + \"_asm_Trained_NN\"\nprint(\"saving:\", filename, \"...\")\nnet.save(filename)\nprint(\"done.\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 2d. plot comparison curve: accuracy & cost, for both training & evaluation\n#\nplot_curve_comparison(epoch, \"Comparison - Training & Evaluation Accuracy\", \\\n                      \"training\", training_accuracy, \"evaluation\", evaluation_accuracy)\nplot_curve_comparison(epoch, \"Comparison - Training & Evaluation Cost\", \\\n                      \"training\", training_cost, \"evaluation\", evaluation_cost)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 2.e drawing each curve one at a time (optional)\n# #draw the curve - x for epoch and y for training cost\n# plot_curve(\"epoch\", epoch, \"training loss\", training_cost)\n# #draw the curve - x for epoch and y for training accuracy\n# plot_curve(\"epoch\", epoch, \"training accuracy\", training_accuracy)\n# #draw the curve - x for epoch and y for evaluation cost\n# plot_curve(\"epoch\", epoch, \"evaluation loss\", evaluation_cost)\n# #draw the curve - x for epoch and y for evaluation accuracy\n# plot_curve(\"epoch\", epoch, \"evaluation accuracy\", evaluation_accuracy)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 3. PREDICT New Image, based on saved network\n#\n# 3a. 1st, load the previously saved network \nfilename = \"2018624223944_asm_Trained_NN\"\nmynet = network2_asm_b_py3.load(filename)\n\n# 3b. define predict() function\ndef predict(network, x):        \n    # feedforward to predict with trained weights, biases\n    # network: trained neural network, x: new images\n    activation = x\n    activations = [x] # list to store all the activations, layer by layer\n    i=0\n    for b, w in zip(network.biases, network.weights):\n        i=i+1\n        z = np.dot(w, activation)+b\n        activation = sigmoid(z)\n    #extracting highest confidence level from feed forward output\n    digit=0\n    confidence_level = 0.0\n    threshold = 0.5\n    j=0\n    for i in activation: # output from the last layer\n        if i >= threshold:\n            digit=j\n            confidence_level = i\n            break\n        else:\n            j=j+1\n    #print \"predicted digit: {0}, confidence level: {1}%\".format(digit, float(confidence_level)*100)\n    conf_level = float(confidence_level*100)\n    return digit, conf_level\n            \ndef sigmoid(z):\n    \"\"\"The sigmoid function.\"\"\"\n    return 1.0/(1.0+np.exp(-z))\n\n# 3c. then, predict \n# get a digit from new image\nprint(\"A digit to predict from mnist testing dataset\")\n\nrow = 1\ncolumn = 2\n# just resuffle so we are showing different data when refreshed, from mnist test dataset\nnp.random.shuffle(test_data) \nfor i in range(column):\n    #get corresponding target label (y) \"digit\" from its vectorized format\n    digit_label = test_data[i][1]\n    # finally, call predict() to predict a new image\n    image = test_data[i][0] \n    digit_predicted, conf_level = predict(mynet, image)\n    title = \"label:{0}, predicted:{1}, conf_lvl:{2}%\".format(digit_label, \\\n        digit_predicted, float(math.trunc(conf_level*1000))/1000)\n    print(title)\n    #then, plot the image data with title \"labelled digit, predicted digit + its confidence level\" \n    image = test_data[i][0].reshape(28, 28)   \n    plt.subplot(row, column, 1)  # subplot with size  \n    plt.imshow(image, cmap='gray')  # cmap='gray' is for black and white picture.\n    #plt.title(\"label:{0}, predicted:{0}, conf_lvl:{1}%\".format(digit_label, digit_predicted, conf_level))\n    plt.title(title)\n    plt.axis(\"on\") # show axis value\n    plt.tight_layout() # automatic padding between subplots\n    plt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls -al"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}